---

title: 03. Kafka Producers - Writing Messages to Kafka
layout: default
parent: Kafka the definitive guide
grand_parent: Books log

---

## Overview

Send message flow:

![Pasted image 20230213175459.png](/Books%20log/Kafka%20the%20definitive%20guide/img/Pasted%20image%2020230213175459.png)

## Constructing Kafka Producer

### Mandatory properties

*   `bootstrap.servers` - list of host:port pairs to establish initial connection to Kafka. Recommend putting at least 2 servers here
*   `key.serializer` - name of class that will be used to serialize the keys. If it's common type built-in serializers can be used: `String Serializer`, `IntegerSerializer`, [etc.](https://kafka.apache.org/34/javadoc/org/apache/kafka/common/serialization/package-summary.html)
*   `value.serializer` - name of class that will be used to serialize messages.

Setup minimal producer example:

```java
Properties kafkaProps = new Properties();
kafkaProps.put("bootstrap.servers", "broker1:9092,broker2:9092");
kafkaProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
kafkaProps.put("value.serializer",
"org.apache.kafka.common.serialization.StringSerializer");
producer = new KafkaProducer<String, String>(kafkaProps);
```

[Producer configuration properties reference](https://kafka.apache.org/documentation.html#producerconfigs)

## Sending Message to Kafka

### Sending Message Synchronously

Despite Kafka producer is always asynchronous, [`send()`](https://kafka.apache.org/34/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send\(org.apache.kafka.clients.producer.ProducerRecord\))  returns [`Future` object](https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/concurrent/Future.html) on which you can block by calling `.get()`

```java
ProducerRecord<String, String> record =
	new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
	try {
        producer.send(record).get();
    } catch (Exception e) {
        e.printStackTrace();
}
```

`KafkaProducer` has 2 types of error. [Retriable](https://kafka.apache.org/34/javadoc/org/apache/kafka/connect/errors/RetriableException.html) can possibly be solved by sending message again (connection error, partition leader election error, etc.). `KafkaProducer` can be configured to make retry automatically. Some errors will not be resolved by retrying, for example, “Message size too large.” In those cases, `KafkaProducer` will not attempt a retry and will return the exception immediately.

### Sending Message Asynchronously

To send messages asynchronously and still handle error scenarios, the producer supports adding a callback when sending a record with method: [`send(ProducerRecord<K,V> record, Callback callback)`](https://kafka.apache.org/34/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send\(org.apache.kafka.clients.producer.ProducerRecord,org.apache.kafka.clients.producer.Callback\))

```java
private class DemoProducerCallback implements Callback {
    @Override
    public void onCompletion(RecordMetadata recordMetadata, Exception e) {
        if (e != null) {
            e.printStackTrace();
        }
}}

ProducerRecord<String, String> record = new ProducerRecord<>("CustomerCountry", "Biomedical Materials", "USA");

producer.send(record, new DemoProducerCallback());
```

The callbacks execute in the producer’s main thread. This guarantees that when we send two messages to the same partition one after another, their callbacks will be executed in the same order that we sent them. But it also means that the callback should be reasonably fast to avoid delaying the producer and preventing other messages from being sent. It is not recommended to perform a blocking operation within the callback. Instead, you should use another thread to perform any blocking operation concurrently.

## Configuring Producers

[Producer configuration properties reference](https://kafka.apache.org/documentation.html#producerconfigs)

*   `client.id` - logical identifier for the client. Choosing a right name will make troubleshooting easier
*   `acks` - control how many partition replicas must receive a message before producer decide that message is successfully sent. By default, Kafka will wait 1 acknowledgment from the leader partition. Possible value:
    *   acks=0 - no wait at all
    *   acks=1
    *   acks=all

### Message Delivery Time

![Pasted image 20230213184005.png](/Books%20log/Kafka%20the%20definitive%20guide/img/Pasted%20image%2020230213184005.png)

*   `max.block.ms` - how long producer blocks when calling `send()` or `partitionsFor()`. If timeout reach, a timeout exception will be thrown. Those methods may block when the producer’s send buffer is full or when metadata is not available
*   `delivery.timeout.ms` - limit amount of time spent from the point a record is ready for sending until either broker responds or client give up, including time spent on retries
*   `request.timeout.ms` - how long producer will wait reply from the server when sending data
*   `retries and retry.backoff.ms` - num of retries and how long wait before send retry request
*   `linger.ms` - how long to wait before sending new *batch*. Producer send batch either when it's full or `linger.ms` reached
*   `buffer.memory` - if messages send by application faster than producer send them to a broker, producer can run out of space. This parameter configures the size of producer's buffer.
*   `compression.type` - by default messages send uncompressed, possible values: snappy, gzip, lz4, or zstd
*   `batch.size` - amount of memory in bytes that be used for each batch
*   `max.in.flight.requests.per.connection` - how many messages producer will send without receiving responses. Default value is 5. This parameter can have impact on messages order in some cases. If `retries` non-zero and `max.in.flight.requests.per.connection` gt 1, in case of fail, the earliest message can be handled later. To prevent this, `enable.idempotence=true` can be used, but it requires to `max.in.flight.requests.per.connection` be lt or eq to 5
*   `max.request.size` - maximum size of single request that can be sent by producer. It may be request with single message or request with batch of messages. This parameter applies to the whole request. In addition, the broker has its own limit on the size of the largest message it will accept. It is usually a good idea to have these configurations match, so the producer will not attempt to send messages of a size that will be rejected by the broker
*   `enable.idempotence` - if set to true, the producer will attach a sequence number to each record it sends and when the broker receives records with the same sequence number, it will reject the second copy and the producer will receive the DuplicateSequenceException.

## Custom Serializers

Custom serializer can be created by implementing interface [`Serializer<T>`](https://kafka.apache.org/34/javadoc/org/apache/kafka/common/serialization/Serializer.html) but it's better to use well known formats like [JSON, Avro, ProtoBuff etc. which already have serializers](https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#formats-serializers-and-deserializers)

## Partitions

Kafka message is a key-value pair, but key is a bit strange: it can be null or repeats. Usually key serves 2 goal: add additional information to the message and be used to decide which partition message should be placed.

*   By default, all messages with the same key will go to the same partition, until the key isn't a null the number of partitions hasn't changed since the last message with the same key was sent
*   If key is null and default partitioner is used - the message will be placed to one of available partition at random
*   If a key exists and the default partitioner is used, Kafka will hash the key and use the result to map the message to a specific partition
*   Kafka also provides  *RoundRobin Partitioner* and *UniformStickyPartitioner*
*   To create custom partitioner [`Partitioner`](https://kafka.apache.org/34/javadoc/org/apache/kafka/clients/producer/Partitioner.html) interface must be implemented

[How to Choose the Number of Topics/Partitions in a Kafka Cluster?](https://www.confluent.io/blog/how-choose-number-topics-partitions-kafka-cluster/)

## Headers

Add some metadata about the Kafka record, without adding any extra information to the key/value pair of the record itself

```java
ProducerRecord<String, String> record = new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
record.headers().add("privacy-level","YOLO".getBytes(StandardCharsets.UTF_8));
```

## Interceptors

Modify the behavior of app that use Kafka without changing already created code. Common use cases for producer interceptors include capturing monitoring and tracking information; enhancing the message with standard headers, especially for lineage tracking purposes; and redacting sensitive information.

To create interceptor [ProducerInterceptor\<K,V>](https://kafka.apache.org/34/javadoc/org/apache/kafka/clients/producer/ProducerInterceptor.html) must be implemented.

## Quotas and Throttling

There'e 3 types of quotas in Kafka: producer, consumer and request. The first two limit the rate at which consumer or producer send or receive data, measured in bytes per second. Request quotas limit the percentage of time the broker spends processing client requests.

Quotas can be set as default for all clients, for specific client-ids or specific users. The default produce and consume quotas that are applied to all clients are part of the Kafka broker configuration file and can be changed only by updating the configuration file and restarting the broker. But quotas for specific clients are applying by dynamic configuration.

When a client reaches its quota, the broker will start throttling the client’s requests to prevent it from exceeding the quota. This means that the broker will delay responses to client requests.

The throttling behavior is exposed to clients via `produce-throttle-time-avg`, `produce-throttle-time-max`, `fetch-throttle-time-avg`, and `fetch-throttle-time-max`, the average and the maximum amount of time a produce request and fetch request was delayed due to throttling.
