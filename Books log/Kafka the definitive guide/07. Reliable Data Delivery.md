---

title: 07. Reliable Data Delivery
layout: default
parent: Kafka the definitive guide
grand_parent: Books log

---

## Kafka guarantees

1.  Kafka provides order guarantee of messages in a partition. If message B was written after message A, using the same producer in the same partition, then Kafka guarantees that the offset of message B will be higher than message A, and that consumers will read message B after message A.
2.  Produced messages are considered “committed” when they were written to the partition on all its in-sync replicas (but not necessarily flushed to disk). Producers can choose to receive acknowledgments of sent messages when the message was fully committed, when it was written to the leader, or when it was sent over the network.
3.  Messages that are committed will not be lost as long as at least one replica remains alive.
4.  Consumers can only read messages that are committed.

## Replication

[Hardening Kafka Replication Talk](https://www.confluent.io/kafka-summit-sf18/hardening-kafka-replication/)
[Please Upgrade Apache Kafka Now Talk](https://www.confluent.io/kafka-summit-san-francisco-2019/please-upgrade-apache-kafka-now/)

As a message can be only read by consumers when it's replicated to all replicas, it's very important to have fast replication in order to prevent performance issues. To prevent performance issues if one of replicas are behind a leader, Kafka has *in-sync* and *out-of-sync* replica's status.

Replica is *in-sync* if:

1.  Has an active session with ZooKeeper—meaning that it sent a heartbeat to ZooKeeper in the last 6 seconds (configurable)
2.  Fetched messages from the leader in the last 10 seconds (configurable)
3.  Fetched the most recent messages from the leader in the last 10 seconds. That is, it isn’t enough that the follower is still getting messages from the leader; it must have had no lag at least once in the last 10 seconds (configurable)

If replica become *out-of-sync* it can return to *in-sync* after it connects to ZooKeeper and catches up the most recent messages.

## Broker Configuration

### Replication Factor

*   `replication.factor` - topic level property
*   `default.replication.factor` - broker level property for automatically created topics

Key consideration in process of deciding how many replicas is needed:

1.  Availability - the more replicas we have, the higher availability we can expect
2.  Durability - with more copies, especially on different storage devices, the probability of losing all of them is reduced
3.  Throughput - with each additional replica, we multiply the inter-broker traffic
4.  End-to-end latency - each produced record has to be replicated to all in-sync replicas before it is available for consumers
5.  Cost - the more replicas we have of our data, the higher the storage and network costs. Since many storage systems already replicate each block 3 times, it sometimes makes sense to reduce costs by configuring Kafka with a replication factor of 2

Placement of replicas also very important factor. If all broker in the same rack all of them become unavailable in case of problem with rack switch.

### Unclean Leader Election

*   `unclean.leader.election.enable` - broker level property, *false* by default

Property control what to do in case when only *out-of-sync* remains available. There're 2 type of this situation:

1.  The partition had three replicas, and the two followers became unavailable (let’s say two brokers crashed). In this situation, as producers continue writing to the leader, all the messages are acknowledged and committed (since the leader is the one and only in-sync replica). Now let’s say that the leader becomes unavailable (oops, another broker crash). In this scenario, if one of the out-of-sync followers starts first, we have an out-of-sync replica as the only available replica for the partition
2.  The partition had three replicas, and due to network issues, the two followers fell behind so that even though they are up and replicating, they are no longer *in-sync*. The leader keeps accepting messages as the only in-sync replica. Now if the leader becomes unavailable, there are only out-of-sync replicas available to become leaders.
    In both scenarios we have a choice:
3.  Elect *out-of-sync* replica as a new leader and lost messages
4.  Wait while *in-sync* replica become available again

To allow *out-of-sync* replicas become a leader we can set `unclean.leader.election.enable` to *true*.

### Minimum In-Sync Replicas

*   `min.insync.replicas` - broker and topic level property

Control how many *in-sync* replicas must be available for the broker to accept messages from producer. If not enough *in-sync* replicas `NotEnoughReplicasException` will be thrown.

### Keeping Replicas In Sync

*   `zookeeper.session.timeout.ms` - time interval during which broker can stop sending heartbeats to ZooKeeper without ZooKeeper considering the broker dead and removing it from the cluster
*   `replica.lag.time.max.ms` - time interval during in which if a follower hasn’t sent any fetch requests or hasn’t consumed up to the leaders log end offset it will become *out-of-sync*

### Persisting to Disk

*   `flush.messages` - control the maximum number of messages not synced to disk
*   `flush.ms` - control the frequency of syncing to disk
    By default Kafka will flush messages to disk when rotating segments (by default 1 GB in size) and before restarts but will otherwise rely on Linux page cache to flush messages when it becomes full

## Using Producers in a Reliable System

Two important parts on coding reliable producer:

1.  Use the correct acks configuration to match reliability requirements.
    Combination of `acks=all` and configured `min.insync.replicas` to more than 1 is the safest option.

2.  Handle errors correctly, both in configuration and in code.
    There are 2 types of errors: *retriable* and not. If the goal is never lose message, the best approach is configuring producer to keep trying to send the message when it encounters a retriable error: leave max retries with default `MAX_INT` value and configure `delivery.timout.ms` properly. This approach can help reach *At least once* guarantee. If *exactly once* is required `enable.idempotence=true` also should be configured.

## Using Consumers in a Reliable System

### Important Consumer Configuration Properties for Reliable Processing

`group.id` - the basic idea is that if two consumers have the same group ID and subscribe to the same topic, each will be assigned a subset of the partitions in the topic and will therefore only read a subset of the messages individually (but all the messages will be read by the group as a whole). If we need a consumer to see, on its own, every single message in the topics it is subscribed to, it will need a unique group.id.

`auto.offset.reset` - This parameter controls what the consumer will do when no offsets were committed (e.g., when the consumer first starts) or when the consumer asks for offsets that don’t exist in the broker.
1\. earliest - the consumer will start from the beginning of the partition whenever it doesn’t have a valid offset. This can lead to a duplication.
2\. latest - the consumer will start at the end of the partition. This can lead to a message lost.

`enable.auto.commit` - automatic offset commit guarantees we will never accidentally commit an offset that we didn’t process. The main drawbacks of automatic offset commits is that we have no control over the number of duplicate records the application may process because it was stopped after processing some records but before the automated commit kicked in.

`auto.commit.interval.ms` - if we choose to commit offsets automatically, this configuration lets us configure how frequently they will be committed.

### Explicitly Committing Offsets in Consumers

1.  Always commit offsets after messages were processed
2.  Commit frequency is a trade-off between performance and number of duplicates in the event of a crash
3.  Rebalances - when designing an application, we need to remember that consumer rebalances will happen, properly handling this cases involves committing offsets before partitions are revoked and cleaning any state the application maintains when it is assigned new partitions
4.  Consumers may need to retry - in some cases consumer may hasn't handled earlier message but has handled later message(e.g. database wasn't available). In this case if commit happen, the earliest message can remain unhandled, there're 2 common patterns how to solve this:
    1.  Commit latest processed message. Pause the consumer. Retry process message
    2.  Write message to a separate topic to handle it later
5.  Consumers may need to maintain state

## Validating System Reliability

### Validating Configuration

Kafka includes two important tools to help with this validation. The `org.apache.kafka.tools` package includes `VerifiableProducer` and `VerifiableConsumer` classes. These can run as command-line tools or be embedded in an automated testing framework.

[System Integration & Performance Testing](https://github.com/apache/kafka/tree/trunk/tests)

### Validating Applications

What to test?
Custom error-handling code, offset commits, and rebalance listeners

Recommended conditions to test app:

*   Clients lose connectivity to one of the brokers
*   High latency between client and broker
*   Disk full
*   Hanging disk (also called “brown out”)
*   Leader election
*   Rolling restart of brokers
*   Rolling restart of consumers
*   Rolling restart of producers

Example of tool for *fault injection*: [Trogdor](https://github.com/apache/kafka/blob/trunk/TROGDOR.md)

### Monitoring Reliability in Production

Kafka’s Java clients include JMX metrics that allow monitoring client-side status and events.

Producer's important metrics:

*   error-rate
*   retry-rate per period
    Also producer's log can be monitored.

Consumer's important metrics:

*   consumer lag - metric indicates how far the consumer is from the latest message committed to the partition

In addition to monitoring clients and the end-to-end flow of data, Kafka brokers include metrics that indicate the rate of error responses sent from the brokers to clients:

*   `kafka.server:type=BrokerTopicMetrics, name=FailedProduceRequestsPerSec`
*   `kafka.server:type=BrokerTopic Metrics, name=FailedFetchRequestsPerSec`
