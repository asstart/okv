---

title: 06. Kafka Internals
layout: default
parent: Kafka the definitive guide
grand_parent: Books log

---

## Cluster Membership

ZooKeeper is used to maintain a list of brokers. Every broker registered in ZooKeeper as [ephemeral node](https://zookeeper.apache.org/doc/r3.4.8/zookeeperProgrammers.html#Ephemeral+Nodes), so when a broker loses connectivity to ZooKeeper this node will be automatically removed from ZK and Kafka components that are watching the list of brokers will be notified about it.

## Controller

Controller is the Kafka broker is responsible for *electing partition leaders*. The first broker that starts in the cluster become a controller by creating *ephemeral node* in `/controller`.

### New controller election

If controller broker loses connectivity - ephemeral node will be deleted and other brokers will receive notification and will attempt to create a new controller node. Each time new controller elected it receives *controller epoch* number. As brokers know current controller epoch, if they receive message from a controller with older number they will ignore it. It needs to prevent split brain scenario.

When new broker become a controller it need to read latest replica state from ZK before it can start managing cluster - big number of partitions in cluster can significantly affect this stage.

### New partition leader election

1.  Controller get notification that broker left the cluster (by watching relevant path or getting [`ControlledShutdown`](https://kafka.apache.org/protocol.html#The_Messages_ControlledShutdown) request)
2.  Controller goes over all the partitions that need a new leader and determine who the new leader should be (next replica in the replica list of that partition)
3.  Controller persist new state to ZK (async)
4.  Controller send [`LeaderAndISR`](https://kafka.apache.org/protocol.html#The_Messages_LeaderAndIsr) request to all brokers that contains replicas for those partitions
5.  Controller send [`UpdateMetadata`](https://kafka.apache.org/protocol.html#The_Messages_UpdateMetadata) request to all brokers to update `MetadataCache`

## Replication

2 types of replicas:

1.  Leader replica - each partition has a single one. Its responsibilities:
    1.  Handle produce and consume requests
    2.  Know which follower is up-to-date with leader. Followers syncing with leader consume messages in the same way as consumers consume messages. So the leader can always know in which state each replica is by offset. If replica inactive or behind for the amount of time set in `replica.lag.time.max.ms` (10 sec by default) it becomes out of sync and can't be used in leader in case of new election
2.  Follower replica - by default doesn't serve any client requests. To start, servers consume requests consumers need to configure `client.rack` and broker needs to update `replica.selector.class` from `LeaderSelector` to `RackAwareReplicaSelector`

## Request Processing

[Kafka clients](https://cwiki.apache.org/confluence/display/KAFKA/Clients)
[Kafka binary protocol](https://kafka.apache.org/protocol.html)
[Apache Kafka, Purgatory, and Hierarchical Timing Wheels](https://www.confluent.io/blog/apache-kafka-purgatory-hierarchical-timing-wheels/)

All requests sent to the broker from a specific client will be processed in the order in which they were received.

All requests have a standard header that includes:

*   Request type (also called API key)
*   Request version (so the brokers can handle clients of different versions and respond accordingly)
*   Correlation ID: a number that uniquely identifies the request and also appears in the response and in the error logs (the ID is used for troubleshooting)
*   Client ID: used to identify the application that sent the request

### Request handling flow

![Pasted image 20230216185516.png](/Books%20log/Kafka%20the%20definitive%20guide/img/Pasted%20image%2020230216185516.png)

1.  *Network threads*:
    1.  take requests from clients and put to the *Request queue*
    2.  take responses from the *Response queue* and send to clients. As responses must be delivered to clients only when data is available - [purgatory](https://www.confluent.io/blog/apache-kafka-purgatory-hierarchical-timing-wheels/) pattern is used to control this delay
2.  *IO threads* responsible for handling requests

### Metadata Requests

Clients somehow need to find out where to send their requests? For this, *Metadata requests* is used. This requests can be sent to any broker, because every broker hold *Metadata cache*. Clients also cache this on their side and update with an interval (controlled with `metadata. max.age.ms` param) or if *Not a leader response* have gotten.

### Produce Requests

1.  When broker receives *produce request* it starts a series of validations:
    1.  Does user have write permissions on a topic?
    2.  Is `acks` value valid? (allowed: 0,1 and all)
    3.  if `acks=all` are there enough *in-sync* replicas? Broker can be configured to refuse this messages if they can'be replicated properly
2.  Broker write a new message to local disk
3.  Depends on `acks` value broker will either respond now(in case of `acks=[0,1]`) or put message into *purgatory*

> TODO does `acks=1` means that message is accepted only on the leader partition, not on leader + 1 follower?

### Fetch Requests

Clients can specify for those requests:
1\. Max amount of data that will be returned (upper boundary)
2\. Min amount of data that will be returned (lower boundary)
3\. Number of messages that will be returned
4\. Amount of time to wait requested amount of data

Kafka uses *zero-copy* method to send data directly from the disk without intermediate buffers.

Consumers are able to read messages that were replicated to all replicas.

If consumer asked to read a message that already deleted or offset doesn't exist, yet error will be returned.

### Other Requests

[List of Kafka's request types](https://kafka.apache.org/protocol.html#protocol_api_keys)

## Physical Storage

Basic storage unit is *partition*:

1.  Can't be split between multiple brokers
2.  Can't be split between multiple disks on the same broker

> TODO what is log segment then?

### Tiered Storage

In the tiered storage approach, the Kafka cluster is configured with two tiers of storage: local and remote.

The local tier is the same as the current Kafka storage tier—it uses the local disks on the Kafka brokers. The remote tier uses dedicated storage systems, such as HDFS or S3, to store the completed log segments.

Separate storage retention policy can be configured for each tier.

Local storage has a lower latency.

### Partition Allocation

Allocations goals:

1.  Spread replicas evenly across brokers
2.  Make sure for each partition each replica on different broker
3.  If broker has rack information, try to assign each replica on broker in different rack

If broker doesn't rack aware - round robin will be used.

If broker is rack aware also round robin will be used, but the order of brokers will different: 0.  suppose that we know that brokers 0 and 1 are on the same rack, and brokers 2 and 3 are on a separate rack. Instead of picking brokers in the order of 0 to 3, we order them as 0, 2, 1, 3—each broker is followed by a broker from a different rack.

Allocation of partitions to brokers does not take available space or existing load into account, and that allocation of partitions to disks takes the number of partitions into account but not the size of the partitions.

### File Management

*Segment* - single file within partition. By default, contains 1GB or 1 week of data. The segment we are currently writing to is called an *active segment*. Active segment is never deleted.

### File Format

Message format stored on disk is the same as the message format sending between broker and produce/consumer.

Kafka always sends messages in batches, so if request contains more than 1 message in it, it can perform better, because batch metadata will be sent only once for this batch.

Message batch headers:

1.  message format version (actual v2)
2.  offset with batch
3.  timestamp of the first message
4.  size of batch in bytes
5.  epoch of the leader that received batch
6.  batch checksum
7.  16 bit indicating: compression type, timestamp, whether batch is part of a transaction or a control batch
8.  produced id, producer epoch, first sequence in the batch
9.  set of messages that are part of the batch

Message system headers:

1.  Size of the record in bytes
2.  Diff between current record and the first offset in the batch
3.  The user payload: key, value, headers

To see all detailed information with metadata `DumpLogSegment` can be used with partition segment: `bin/kafka-run-class.sh kafka.tools.DumpLogSegments`

### Indexes

Kafka index messages by offset and timestamp.
Indexes are broken into segments - so an old index can be deleted when there's no more messages in a segment.
Index segment can be safely deleted, in this case it will be regenerated automatically.

### Compaction

`log.cleaner.enabled` -property to enable compaction

During the compaction process, each log is viewed as split into 2 parts:

1.  Clean - contains message compacted before
2.  Dirty - message were written after last compaction

To compact a partition, the cleaner thread reads the dirty section of the partition and creates an in-memory map. Each map entry is comprised of a 16-byte hash of a message key and the 8-byte offset of the previous message that had this same key. This means each map entry only uses 24 bytes.

At least one full segment's map must fit in memory allocated for compacting threads.

Once the cleaner thread builds the offset map, it will start reading off the clean segments, starting with the oldest, and check their contents against the offset map. For each message, it checks if the key of the message exists in the offset map. If the key does not exist in the map, the value of the message just read is still the latest, and the message is copied over to a replacement segment. If the key does exist in the map, the message is omitted because there is a message with an identical key but newer value later in the partition.
