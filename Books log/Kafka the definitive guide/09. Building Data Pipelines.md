---

title: 09. Building Data Pipelines
layout: default
parent: Kafka the definitive guide
grand_parent: Books log

---

Kafka Connect – part of Apache Kafka that provides a scalable and reliable way to move data between Kafka and other data sources. It provides APIs and runtime to develop and run plugins.

Kafka Connectors run as a cluster of *worker* processes.

You install a plugin to the workers and then use a REST API to configure and manage *connectors*.

Connectors start *tasks* to move data.

There are 2 type of connectors:

*   Source connectors - responsible for reading data from a source system
*   Sink connectors - responsible for writing data to a target system

Kafka Connect also support *converters* which make able to store data in Kafka in any format.

## Configure and Run

```shell
bin/connect-distributed.sh config/connect-distributed.properties
```

*   `bootstrap.servers` - list of Kafka brokers
*   `group.id` -
*   `plugin.path` - one or more directories where connectors and their dependencies can be found. Example: `plugin.path=/opt/connectors,/home/gwenshap/connectors.`
*   `key.converter`/`value.converter` - which converter will be used to convert keys and values to store it in Kafka, like: JSONConverter, JscoSchemaConverter, AvroConverter, Protobuf Converter. Also, each converter can have specific configuration options, they can be configured in the following way: \`key.converter/value.converter.<converter key word>.<property>
*   `rest.host.name and rest.port` - specific port to configure and monitor connector through the REST API

## Kafka Connect Internals

### Connectors and tasks

Connector plugins implement the Connector API, which includes two parts:

*   Connectors - responsible for following things:
    *   Determine how many tasks will run for the connector
    *   Deciding how to split the data-copying work between tasks
    *   Getting configuration for the tasks from the workers and passing them along
*   Tasks - responsible for actually getting data in or out of Kafka. All tasks are initialized by receiving context from a worker.
    *   Source context includes an object that allows to store the offset of source records
    *   Sink context includes methods to control the records it's receiving from the Kafka

### Workers

Kafka Connect's worker processes are container processes that execute connector and tasks. While *connectors and tasks* are responsible for moving data, *workers* are responsible for the REST API, configuration management, reliability, high availability, scaling, and load balancing.

### Offset management

The idea of the *offset management* is that connectors need to know which data they have already processed, and they can use APIs provided by Kafka to maintain information on which events were already processed. When the source connector returns a list of records, which includes the source partition and offset for each record, the worker sends the records to Kafka brokers. If the brokers successfully acknowledge the records, the worker then stores the offsets of the records it sent to Kafka. This allows connectors to start processing events from the most recently stored offset after a restart or a crash. The storage mechanism is pluggable and is usually a Kafka topic; you can control the topic name with the `offset.storage.topic` configuration.

Sink connectors have an opposite but similar workflow: they read Kafka records, which already have a topic, partition, and offset identifiers. Then they call the connector `put()` method that should store those records in the destination system. If the connector reports success, they commit the offsets they’ve given to the connector back to Kafka, using the usual consumer commit methods.
