---

title: 13. Monitoring Kafka
layout: default
parent: Kafka the definitive guide
grand_parent: Books log

---

## Kafka Broker Metrics

Types of metrics that can be monitored:

*   [Broker metrics](https://docs.confluent.io/platform/current/kafka/monitoring.html#server-metrics)
*   [ZooKeeper metrics](https://docs.confluent.io/platform/current/kafka/monitoring.html#zk-metrics)
*   [JVM metrics](https://sysdig.com/blog/jmx-monitoring-custom-metrics/)
*   OS metrics
*   Logs

### Under-Replicated Partitions

One of the most popular metrics to use when monitoring Kafka is under-replicated partitions.

A steady (unchanging) number of under-replicated partitions reported by many of the brokers in a cluster normally indicates that one of the brokers in the cluster is offline. The count of under-replicated partitions across the entire cluster will equal the number of partitions that are assigned to that broker, and the broker that is down will not report a metric.

If the number of under-replicated partitions is fluctuating, or if the number is steady, but there are no brokers offline, this typically indicates a performance issue in the cluster.

#### Cluster-Level Problems

*   Unbalanced loads - following metrics can help to detect this problem:
    *   Partition count
    *   Leader partition count
    *   All topics messages in rate
    *   All topics bytes in rate
    *   All topics bytes out rate
        In a perfectly balanced cluster - all these metrics will be even across all brokers. If load isn't even, partitions can be moved manually by using `kafka-reassign-partitions.sh`
*   Resource exhaustion - CPU, IO and network throughput are the most common bottlenecks that could slow down a cluster. Following metrics can help to detect resource connected problem:
    *   CPU utilization
    *   Inbound network throughput
    *   Outbound network throughput
    *   Disk average wait time
    *   Disk percent utilization

#### Host-Level Problems

*   Hardware failures
    *   CPU/Memory failures
        *   Monitoring hardware status from IPMI
    *   Disk failure
        *   Monitoring hardware status from IPMI
        *   SMART (Self-Monitoring, Analysis and Reporting Technology) should be running
        *   Keep an eye on the disk controller
*   Networking
    *   Hardware issues, such as a bad network cable or connector
    *   Configuration issues
        *   Monitoring number of errors detected on the network interfaces
*   Conflicts with another process
    *   Tools, such as top, can help to identify if there is a process that is using more CPU or memory than expected
*   Local configuration differences

### Broker Metrics

This is a short list with examples of Kafka JMX metrics, list of all metrics can be found here: [Monitoring Kafka with JMX](https://docs.confluent.io/platform/current/kafka/monitoring.html#monitoring-ak-with-jmx)

*   Active controller count - metric indicates whether the broker is currently the controller for the cluster. The metric will either be 0 or 1, with 1 showing that the broker is currently the controller. At all times, only one broker should be the controller, and one broker must always be the controller in the cluster
    \| JMX MBean | kafka.controller:type=KafkaController,name=ActiveControllerCount |
    \|---|---|
    |Value range|0,1|

*   Controller queue size - metric indicates howmany requests the controller is currently waiting to process for the brokers
    |JMX MBean|kafka.controller:type=ControllerEventManager,name=EventQueueSize|
    \|---|---|
    |Value range|>=0|

*   Request handler idle ratio - Kafka uses two thread pools for handling all client requests: network threads and request handler threads. The network threads are responsible for reading and writing data to the clients across the network. The request handler threads are responsible for servicing the client request itself, which includes reading or writing the messages to disk.
    |JMX MBean|kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent|
    \|---|---|
    |Value range|float in range 0-1|

*   All topics bytes in - the all topics bytes in rate, expressed in bytes per second, is useful as a measurement of how much message traffic your brokers are receiving from producing clients
    |JMX MBean|kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec|
    \|---|---|
    |Value range|rates as doubles, count as integer|

*   All topics bytes out - the all topics bytes out rate, similar to the bytes in rate
    |JMX MBean|kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec|
    \|---|---|
    |Value range|rates as doubles, count as integer|

*   All topics messages in - while the byte rates show the broker traffic in absolute terms of bytes, the messages in rate shows the number of individual messages, regardless of their size, produced per second
    |JMX MBean|kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec|
    \|---|---|
    |Value range|rates as doubles, count as integer|

*   Partition count
    |JMX MBean|kafka.server:type=ReplicaManager,name=PartitionCount|
    \|---|---|
    |Value range|>=0|

### OS Monitoring

*   CPU usage - cpu usage can be observed with the following actions:
    *   Time spent in user space
    *   Time spent in kernel space
    *   Time spent on low-priority processes
    *   Time spent idle
    *   Time spent in wait
    *   Time spent handling hardware interrupts
    *   Time spent handling software interrupts
    *   Time spent waiting for hypervisor
*   Memory usage
*   Disk usage
*   Disk IO
*   Network usage

### Logging

There are two loggers writing to separate files on disk:

*   kafka.controller - is used to provide messages specifically regarding the cluster controller. The information includes topic creation and modification, broker status changes, and cluster activities such as preferred replica elections and partition moves
*   kafka.server.ClientQuotaManager - is used to show messages related to produce and consume quota activities

Enabling the kafka.log.LogCleaner, kafka.log.Cleaner, and kafka.log.LogCleanerManager loggers at the DEBUG level will output information about the status of log compaction threads.

kafka.request.logger turned on at either the DEBUG or TRACE levels will log information about every request sent to the broker

## Client monitoring

As there are many Kafka clients developed on different language, each implementation can have personal approach for monitoring on a client side.

If it's official, Kafka client metrics will be also collected through JMX:

*   Producer metrics - metrics can show overall statistic and per-topic, per-partition statistic. All metric documentation can be found [here](https://kafka.apache.org/documentation/#producer_monitoring). There are few examples of metrics:
    *   record-error-rate
    *   record-retry-rate
    *   request-latency-avg
    *   outgoing-byte-rate
    *   record-send-rate
    *   request-rate
    *   request-size-avg
    *   batch-size-avg
    *   record-size-avg
    *   records-per-request-avg
*   Consumer metrics - full documentation can be found [here](https://kafka.apache.org/documentation/#consumer_monitoring). There are also a few examples of metrics:
    *   fetch-latency-avg
    *   request-latency-avg
    *   fetch.min.bytes
    *   fetch.max.wait.ms
    *   bytes-consumed-rate
    *   records-consumed-rate

### Quotas

Apache Kafka has the ability to throttle client requests in order to prevent one client from overwhelming the entire cluster. The Kafka broker does not use error codes in the response to indicate that the client is being throttled. This means that it is not obvious to the application that throttling is happening without monitoring the metrics that are provided to show the amount of time that the client is being throttled:
\- `bean kafka.consumer:type=consumer-fetch-manager-metrics,client-id=CLIENTID, attribute fetch-throttle-time-avg`
\- `beankafka.producer:type=producer-metrics,client-id=CLIENTID, attributeproduce- throttle-time-avg`

## Lag monitoring

For Kafka consumers, the most important thing to monitor is the consumer lag. `records-lag-max` metric from the consumer client will provide a partial view of the consumer status, but it only represents a single partition, the one that has the most lag, so it does not accurately show how far behind the consumer is. In addition, it requires proper operation of the consumer, because the metric is calculated by the consumer on each fetch request. If the consumer is broken or offline, the metric is either inaccurate or not available. It's better to use external tools like [Burrow](https://github.com/linkedin/Burrow) to monitoring lag.

## End-to-End Monitoring

Another type of external monitoring that is recommended to determine if your Kafka clusters are working properly is an end-to-end monitoring system that provides a client point of view on the health of the Kafka cluster. Tools like [Xinfra Monitor](https://github.com/linkedin/kafka-monitor) can do it. This tool continually produces and consumes data from a topic that is spread across all brokers in a cluster. It measures the availability of both produce and consume requests on each broker, as well as the total produce to consume latency
