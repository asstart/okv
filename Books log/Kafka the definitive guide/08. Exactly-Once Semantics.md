---

title: 08. Exactly-Once Semantics
layout: default
parent: Kafka the definitive guide
grand_parent: Books log

---

Exactly-once semantics if Kafka is a cmbination of 2 features: idempotent producers and transactional semantics.

## Idempotent Producer

The most basic cases of appearing duplicate messages is: producer sent message to a broker, message was replicated successfully, broker crashed before sending ackn to producer, producer will retry sending message.
Kafka can solve this case by using idempotent producers.

### How to use?

1.  Set `enable.idempotence=true` in producer configuration(if producer already configured `acks=all`, there will be no impact on performance)
2.  Ensure that  `max.inflight.requests` is set to 5 or lower
3.  These things will be changed:
    1.  Producer will make one extra API call during initialization to obtain id
    2.  Each batch will contain extra 96 bits of information: producer id and sequence number of the first message in a batch (no need to store it for each message, they can be calculated)
    3.  Broker will validate sequence number

### How does the idempotent producer work?

With turned on `enable.idempotence` message will also include: producer ID(PID) and sequence number. Alongside with topic and target partition, these 4 attributes can uniquely identify a message. Broker use this to track last five message producer to every partition (Not configurable?). Producer also requires setting `max.inflight.requests` to 5 or lower.

> What if broker received message accepted before?
> Reject with error
> Error will log by producer and reflected in the metric `record-error-rate`
> On the broker, this will be reflected in the metric `ErrorsPerSec`

> What if a broker received a message with unexpectedly high sequence number, like: 2,3,30?
> Broker will respond with error *out of order sequence* and continue to operate normally

### How does idempotent work in case of failures?

##### Producer restart

Each initialization of producer ends up that new producer will have different producer id, so in this case broker won't be able to detect duplication.

##### Broker failure

Follower brokers also keep in memory last 5 messages, it means, if a follower become a new leader, it can continue deduplication.

### Limitations

This mechanism will only work in case of Kafka's producer built-in retries. In case of call `send()` 2 time with the same message - message will get different sequence number and won't be deduplicated. The same if 2 different producers send the same message - messages will contain different producer id and also won't be deduplicated.

So it's better to use built-in retry mechanism.

## Transaction

### How to use?

1.  With Kafka Streams
    1.  set `processing.guarantee=exactly_once`
2.  Without Kafka Streams
    1.  Configure producer with `transactional.id`. It should be unique and long-lived
    2.  Configure consumer with `enable.auto.commit=false`
    3.  Configure consumer with `isolation.level=read_committed`

Kafka transactions are developed for use in stream processing and pattern *consumer-transform-produce* and can guarantee *exactly once* semantic in this context. Exactly once, processing means that *consuming*, *transforming* and *producing* are done atomically. So either the offset of original message is committed, and the result is successfully produced of neither of these things happened. To support this, Kafka introduced *atomic multipartitions write*. The main idea of this, is that committing offset and producing result both involve writing messages to partitions.

To enable transaction, a transactional producer must be used. Unlike a non-transactional producer, it also has a *transactional.id* property which persists between producer restarts. Kafka broker also maintains mapping for *transactional.id* and *producer.id*, so in case of restart, producer will get the same *producer.id*. *Transactional.id* alongside with *epoch* number involved in preventing zombie instances from writing to output.

Despite transactions is a producer feature for the most parts, for a proper exactly once semantic, consumer also must be configured properly:

*   `isolation.level` must be set to `read_committed`
    After subscribing to a topic with configured `isolation.level` consumer will receive only messages that were successfully committed or were non-transactional. So it means, if a consumer isn't configured properly, it can receive messages from uncommitted or aborted transactions.

To guarantee message order, consumer will not receive messages that were produced after the point when the first-still-open transaction began (known as Last Stable Offset(LSO)). This messages will be held until that transaction is committed, aborted or aborted automatically by timeout (configured with `transaction.timeout.ms` and 15 min by default).

![Pasted image 20230322170041.png](/Books%20log/Kafka%20the%20definitive%20guide/img/Pasted%20image%2020230322170041.png)

### What Kafka transactions aren't solving?

*   Side effects while stream processing. If the *transform* step involves some side effect activity, like sending email/notification or writing to a file, there aren't any guarantees that these actions will be performed once
*   Reading from a Kafka topic and writing to a database
*   Reading data from a database, writing to Kafka, and from there writing to another database
*   Copying data from one Kafka cluster to another
*   Publish/subscribe pattern - it's a subtle case. While transaction, in this case can guarantee that consumer (configured with *read-committed*) won't see any messages that were part of aborted or not-yet-committed transaction, but there's no any guarantees that this messages will be consumed exactly once

## Performance of Transactions

Transactions add moderate overhead to the producer. The request to register transactional ID occurs once in the producer lifecycle. Additional calls to register partitions as part of a transaction happen at most one per partition for each transaction, then each transaction sends a commit request, which causes an extra commit marker to be written on each partition. The transactional initialization and transaction commit requests are synchronous, so no data will be sent until they complete successfully, fail, or time out, which further increases the overhead.

On the consumer side, there is some overhead involved in reading commit markers. The key impact that transactions have on consumer performance is introduced by the fact that consumers in read\_committed mode will not return records that are part of an open transaction. Long intervals between transaction commits mean that the con‚Äê sumer will need to wait longer before returning messages, and as a result, end-to-end latency will increase.
