---

title: 03. Designing Good Data Architecture
layout: default
parent: Fundamental of data engineering
grand_parent: Books log

---

> Enterprise architecture is the design of systems to support change in the enterprise, achieved by flexible and reversible decisions reached through careful evaluation of trade-offs.

> Data architecture is the design of systems to support the evolving data needs of an enterprise, achieved by flexible and reversible decisions reached through a careful evaluation of trade-offs.

> Bad data architecture is tightly coupled, rigid, overly centralized, or uses the wrong tools for the job, hampering development and change management. Ideally, by designing architecture with reversibility in mind, changes will be less costly.

## Principles of Good Data Architecture

##### 1. Choose Common Components Wisely

Common components can be anything that has broad applicability within an organization like object storage, version-control systems, observability, monitoring and orchestration systems, and processing engines.

Choosing common components is a balancing act. On the one hand, you need to focus on needs across the data engineering lifecycle and teams, utilize common components that will be useful for individual projects, and simultaneously facilitate interoperation and collaboration. On the other hand, architects should avoid decisions that will hamper the productivity of engineers working on domain-specific problems by forcing them into one-size-fits-all technology solutions

##### 2. Plan for Failure

##### 3. Architect for Scalability

##### 4. Architecture Is Leadership

##### 5. Always Be Architecting

Data architects don’t serve in their role simply to maintain the existing state; instead, they constantly design new and exciting things in response to changes in business and technology.

##### 6. Build Loosely Coupled Systems

For software architecture, a loosely coupled system has the following properties:

1.  Systems are broken into many small components.
2.  These systems interface with other services through abstraction layers, such as a messaging bus or an API. These abstraction layers hide and protect internal details of the service, such as a database backend or internal classes and method calls.
3.  As a consequence of property 2, internal changes to a system component don’t require changes in other parts. Details of code updates are hidden behind stable APIs. Each piece can evolve and improve separately.
4.  As a consequence of property 3, there is no waterfall, global release cycle for the whole system. Instead, each component is updated separately as changes and improvements are made.

##### 7. Make Reversible Decisions

##### 8. Prioritize Security

There are two main ideas: zero-trust security and the shared responsibility security model.

Zero-trust security:

> Traditional architectures place a lot of faith in perimeter security, crudely a hardened network perimeter with “trusted things” inside and “untrusted things” outside. Unfortunately, this approach has always been vulnerable to insider attacks, as well as external threats such as spear phishing.

Shared security:
All cloud providers operate on some form of this shared responsibility model. They secure their services according to published specifications. Still, it is ultimately the user’s responsibility to design a security model for their applications and data and leverage cloud capabilities to realize this model.

##### 9. Embrace FinOps

In an on-premises setting, data systems are generally acquired, with a capital expenditure for a new system every few years in an on-premises setting. Responsible parties have to balance their budget against desired compute and storage capacity.

In the cloud era, most data systems are pay-as-you-go and readily scalable. Systems can run on a cost-per-query model, cost-per-processing-capacity model, or another variant of a pay-as-you-go model.

With FinOps, engineers need to learn to think about the cost structures of cloud systems. For example, what is the appropriate mix of AWS spot instances when running a distributed cluster? What is the most appropriate approach for running a sizable daily job in terms of cost-effectiveness and performance? When should the company switch from a pay-per-query model to reserved capacity?

Ops teams should also think in terms of cost attacks.

## Major Architecture Concepts

### Domains and Services

> A domain is the real-world subject area for which you’re architecting.

> A service is a set of functionality whose goal is to accomplish a task.

A domain can contain multiple services. Domains may also share services.

Identify what should go in the domain. When determining what the domain should encompass and what services to include, the best advice is to simply go and talk with users and stakeholders, listen to what they’re saying, and build the services that will help them do their job. Avoid the classic trap of architecting in a vacuum.

### Distributed Systems, Scalability, and Designing for Failure

> Scalability
> Allows us to increase the capacity of a system to improve performance and handle the demand. For example, we might want to scale a system to handle a high rate of queries or process a huge data set.

> Elasticity
> The ability of a scalable system to scale dynamically; a highly elastic system can automatically scale up and down based on the current workload. Scaling up is critical as demand increases, while scaling down saves money in a cloud environment. Modern systems sometimes scale to zero, meaning they can automatically shut down when idle.

> Availability
> The percentage of time an IT service or component is in an operable state.

> Reliability
> The system’s probability of meeting defined standards in performing its intended function during a specified interval.

We utilize a distributed system to realize higher overall scaling capacity and increased availability and reliability. Horizontal scaling allows you to add more machines to satisfy load and resource requirements.

### Tight Versus Loose Coupling: Tiers, Monoliths, and Microservices

Designing “good” data architecture relies on trade-offs between the tight and loose coupling of domains and services.

Data pipelines might consume data from many sources ingested into a central data warehouse. The central data warehouse is inherently monolithic. A move toward a microservices equivalent with a data warehouse is to decouple the workflow with domain-specific data pipelines connecting to corresponding domain-specific data warehouses.

There are several approaches to data pipeline development:

1.  A single team could be  responsible for gathering data from all domains and reconciling it for consumption across the organization. (This is a common approach in traditional data warehousing.)
2.  Another approach is the data mesh. With the data mesh, each software team is responsible for preparing its data for consumption across the rest of the organization.

### User Access: Single Versus Multitenant

As a data engineer, you have to make decisions about sharing systems across multiple teams, organizations, and customers. For example, do multiple departments in a large company share the same data warehouse? Does the organization share data for multiple large customers within the same table?

We have two factors to consider in multitenancy: performance and security.

### Event-Driven Architecture

An event-driven workflow encompasses the ability to create, update, and asynchronously move events across various parts of the data engineering lifecycle. This workflow boils down to three main areas: event production, routing, and consumption. An event must be produced and routed to something that consumes it without tightly coupled dependencies among the producer, event router, and consumer.

The advantage of an event-driven architecture is that it distributes the state of an event across multiple services. This is helpful if a service goes offline, a node fails in a distributed system, or you’d like multiple consumers or services to access the same events. Anytime you have loosely coupled services, this is a candidate for event-driven architecture.

### Brownfield Versus Greenfield Projects

Before you design your data architecture project, you need to know whether you’re starting with a clean slate or redesigning an existing architecture.

Brownfield projects often involve refactoring and reorganizing an existing architecture and are constrained by the choices of the present and past. Because a key part of architecture is change management, you must figure out a way around these limitations and design a path forward to achieve your new business and technical objectives. Brownfield projects require a thorough understanding of the legacy architecture and the interplay of various old and new technologies

Greenfield project allows you to pioneer a fresh start, unconstrained by the history or legacy of a prior architecture.

## Examples and Types of Data Architecture

### Data Warehouse

> A data warehouse is a central data hub used for reporting and analysis. Data in a data warehouse is typically highly formatted and structured for analytics use cases.

Two types of data warehouse architecture: organizational and technical

1.  The organizational data warehouse architecture organizes data associated with certain business team structures and processes. Two main characteristics of organizational data warehouse:
    1.  Separates OLAP from production databases
    2.  Centralizes and organizes data
2.  The technical data warehouse architecture reflects the technical nature of the data warehouse, such as MPP.

### Data Lake

Instead of imposing tight structural limitations on data, why not simply dump all of your data—structured and unstructured into a central location.

Main components of Data Lake:

1.  Storage it could be on-premise HDFS or cloud-based object storage like Amazon S3
2.  Transformation tools: MapReduce, Spark, Hive, Pig, etc.
3.  Schema management
4.  Data Catalog
5.  Discovery Tools

Lack of proper organization of data in Data Lake can easily lead to a *Data Swamp*.

### Convergence, Next-Generation Data Lakes, and the Data Platform

In response to the limitations of first-generation data lakes, various players have sought to enhance the concept to fully realize its promise.

Databricks introduced the notion of a data lakehouse. The lakehouse incorporates the controls, data management, and data structures found in a data warehouse while still housing data in object storage and supporting a variety of query and transformation engines. In particular, the data lakehouse supports ACID transactions, a big departure from the original data lake, where you simply pour in data and never update or delete it. The term data lakehouse suggests a convergence between data lakes and data warehouses.

The technical architecture of cloud data warehouses has evolved to be very similar to a data lake architecture. Cloud data warehouses separate compute from storage, support petabyte-scale queries, store a variety of unstructured data and semistructured objects, and integrate with advanced processing technologies such as Spark or Beam.

### Modern Data Stack

The main objective of the modern data stack is to use cloud-based, plug-and-play, easy-to-use, off-the-shelf components to create a modular and cost-effective data architecture. These components include data pipelines, storage, transformation, data management/governance, monitoring, visualization, and exploration.

### Lambda Architecture

The Lambda architecture was introduced to solve the problem of reconciling batch and streaming data into a single architecture.

In a Lambda architecture, you have systems operating independently of each other—batch, streaming, and serving. The source system is ideally immutable and append-only, sending data to two destinations for processing: stream and batch. In-stream processing intends to serve the data with the lowest possible latency in a “speed” layer, usually a NoSQL database. In the batch layer, data is processed and transformed in a system such as a data warehouse, creating precomputed and aggre‐ gated views of the data. The serving layer provides a combined view by aggregating query results from the two layers.

![Pasted image 20230331163919.png](/Pasted%20image%2020230331163919.png)

### Kappa Architecture

The central thesis is this: why not just use a stream-processing platform as the backbone for all data handling—ingestion, storage, and serving? This facilitates a true event-based architecture. Real-time and batch processing can be applied seamlessly to the same data by reading the live event stream directly and replaying large chunks of data for batch processing.

### The Dataflow Model and Unified Batch and Streaming

One of the central problems of managing batch and stream processing is unifying multiple code paths. While the Kappa architecture relies on a unified queuing and storage layer, one still has to confront using different tools for collecting real-time statistics or running batch aggregation jobs. Today, engineers seek to solve this in several ways. Google made its mark by developing the Dataflow model and the Apache Beam framework that implements this model.

The core idea in the Dataflow model is to view all data as events, as the aggregation is performed over various types of windows. Ongoing real-time event streams are unbounded data. Data batches are simply bounded event streams, and the boundaries provide a natural window. Engineers can choose from various windows for real-time aggregation, such as sliding or tumbling. Real-time and batch processing happens in the same system using nearly identical code.

### Data Mesh

The data mesh attempts to invert the challenges of centralized data architecture, taking the concepts of domain-driven design (commonly used in software architectures) and applying them to data architecture.
